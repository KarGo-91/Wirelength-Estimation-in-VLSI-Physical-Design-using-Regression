{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9364c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import psutil\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "# %% Config\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Top 5 features identified\n",
    "TOP_FEATURES = [\n",
    "    \"DieWidth\",\n",
    "    \"NumInstances\",\n",
    "    \"NumValidPins\", \n",
    "    \"DieHeight\",\n",
    "    \"NumNets\"\n",
    "]\n",
    "\n",
    "TARGET = \"TotalWirelength\"\n",
    "BASE_RESULTS = \"results_top5_ensemble\"\n",
    "os.makedirs(BASE_RESULTS, exist_ok=True)\n",
    "\n",
    "# %% Load and prepare data\n",
    "def load_data():\n",
    "    input_files = [\n",
    "        \"ispd18_global_features.csv\",\n",
    "        \"ispd19_global_features.csv\",\n",
    "        \"N28_global_features.csv\"\n",
    "    ]\n",
    "    df = pd.concat([pd.read_csv(f) for f in input_files], ignore_index=True)\n",
    "    return df[[TARGET] + TOP_FEATURES]\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "# %% Initialize models\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1)\n",
    "xgb = XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=5, random_state=SEED, n_jobs=-1)\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "ensemble = VotingRegressor([(\"RF\", rf), (\"XGB\", xgb), (\"KNN\", knn)], n_jobs=-1)\n",
    "\n",
    "# %% Evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    infer_time = (time.time() - start_time) / len(X_test) * 1000  # ms per sample\n",
    "    \n",
    "    return {\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "        \"InferTime_ms_per_sample\": infer_time\n",
    "    }\n",
    "\n",
    "# %% Memory measurement\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "# %% Main evaluation\n",
    "results = []\n",
    "X = df[TOP_FEATURES]\n",
    "y = df[TARGET]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in [\n",
    "    (\"RandomForest\", rf),\n",
    "    (\"XGBoost\", xgb),\n",
    "    (\"KNN\", knn),\n",
    "    (\"Ensemble\", ensemble)\n",
    "]:\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train model\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Get model size\n",
    "    model_path = os.path.join(BASE_RESULTS, f\"{model_name}.pkl\")\n",
    "    joblib.dump(model, model_path)\n",
    "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    \n",
    "    # Get RAM usage\n",
    "    ram_mb = get_memory_usage()\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"R2\": metrics[\"R2\"],\n",
    "        \"MSE\": metrics[\"MSE\"],\n",
    "        \"Size_MB\": size_mb,\n",
    "        \"InferTime_ms_per_sample\": metrics[\"InferTime_ms_per_sample\"],\n",
    "        \"RAM_MB\": ram_mb,\n",
    "        \"TrainTime_sec\": train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"RÂ²: {metrics['R2']:.4f}\")\n",
    "    print(f\"MSE: {metrics['MSE']:.2f}\")\n",
    "    print(f\"Size: {size_mb:.2f} MB\")\n",
    "    print(f\"Inference Time: {metrics['InferTime_ms_per_sample']:.4f} ms/sample\")\n",
    "    print(f\"RAM Usage: {ram_mb:.2f} MB\")\n",
    "    print(f\"Training Time: {train_time:.2f} sec\")\n",
    "\n",
    "# %% Save results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_csv = os.path.join(BASE_RESULTS, \"model_metrics.csv\")\n",
    "results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(results_df)\n",
    "print(f\"\\nResults saved to: {results_csv}\")\n",
    "\n",
    "# %% Feature Importance Analysis (for ensemble components)\n",
    "print(\"\\n=== Feature Importance ===\")\n",
    "importance_data = []\n",
    "\n",
    "for model_name, model in [(\"RF\", rf), (\"XGB\", xgb)]:\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = model.feature_importances_\n",
    "    \n",
    "    for feature, importance in zip(TOP_FEATURES, importances):\n",
    "        importance_data.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Feature\": feature,\n",
    "            \"Importance\": importance\n",
    "        })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_csv = os.path.join(BASE_RESULTS, \"feature_importance.csv\")\n",
    "importance_df.to_csv(importance_csv, index=False)\n",
    "print(importance_df)\n",
    "print(f\"\\nFeature importance saved to: {importance_csv}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
