{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4956175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading datasets...\n",
      "âœ… Dataset loaded: 8469 rows, 18 columns\n",
      "ðŸŽ¯ Target variable: TotalWirelength\n",
      "âœ‚ Splitting dataset into train and test sets...\n",
      "âœ… Train samples: 6775, Test samples: 1694\n",
      "âš™ Initializing models...\n",
      "âœ… Total models: 18\n",
      "ðŸš€ Starting training for feature subsets...\n",
      "\n",
      "=== Training with top 1 features: ['DieWidth'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.743e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n",
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 1\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 1 features\n",
      "\n",
      "=== Training with top 2 features: ['DieWidth', 'NumInstances'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.152e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.532e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 487\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 2 features\n",
      "\n",
      "=== Training with top 3 features: ['DieWidth', 'NumInstances', 'NumValidPins'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.656e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.950e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 742\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 3 features\n",
      "\n",
      "=== Training with top 4 features: ['DieWidth', 'NumInstances', 'NumValidPins', 'DieHeight'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 4 features\n",
      "\n",
      "=== Training with top 5 features: ['DieWidth', 'NumInstances', 'NumValidPins', 'DieHeight', 'NumNets'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.058e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.058e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1114\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 5 features\n",
      "\n",
      "=== Training with top 6 features: ['DieWidth', 'NumInstances', 'NumValidPins', 'DieHeight', 'NumNets', 'NumMovableInstances'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.055e+25, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1369\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 6 features\n",
      "\n",
      "=== Training with top 7 features: ['DieWidth', 'NumInstances', 'NumValidPins', 'DieHeight', 'NumNets', 'NumMovableInstances', 'NetSkewness'] ===\n",
      "   â–¶ LinearRegression...\n",
      "   â–¶ Ridge...\n",
      "   â–¶ Lasso...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.946e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ ElasticNet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.022e+24, tolerance: 1.637e+22\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ LassoLars...\n",
      "   â–¶ SGDRegressor...\n",
      "   â–¶ RandomForest...\n",
      "   â–¶ GradientBoosting...\n",
      "   â–¶ ExtraTrees...\n",
      "   â–¶ AdaBoost...\n",
      "   â–¶ Bagging...\n",
      "   â–¶ DecisionTree...\n",
      "   â–¶ ExtraTree...\n",
      "   â–¶ KNeighbors...\n",
      "   â–¶ XGBoost...\n",
      "   â–¶ LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1624\n",
      "[LightGBM] [Info] Number of data points in the train set: 6775, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 102882775257.610031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ WeightedVoting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â–¶ Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karishma\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved metrics for 7 features\n",
      "\n",
      "ðŸ“Š Saving RÂ²/MSE comparison table...\n",
      "âœ… All done!\n"
     ]
    }
   ],
   "source": [
    "# %% Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoLars, SGDRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor,\n",
    "    BaggingRegressor, VotingRegressor, StackingRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def wrap_model(model):\n",
    "    \"\"\"Wraps any model in a pipeline with mean imputation.\"\"\"\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "# %% Config\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_RESULTS = \"results_feature_subset\"\n",
    "os.makedirs(BASE_RESULTS, exist_ok=True)\n",
    "for folder in [\"metrics\", \"predictions\", \"plots\"]:\n",
    "    os.makedirs(os.path.join(BASE_RESULTS, folder), exist_ok=True)\n",
    "\n",
    "# Fixed feature order (most â†’ least important)\n",
    "FEATURE_ORDER = [\n",
    "    \"DieWidth\",\n",
    "    \"NumInstances\",\n",
    "    \"NumValidPins\",\n",
    "    \"DieHeight\",\n",
    "    \"NumNets\",\n",
    "    \"NumMovableInstances\",\n",
    "    \"NetSkewness\"\n",
    "]\n",
    "\n",
    "# %% Load Data\n",
    "print(\"ðŸ“‚ Loading datasets...\")\n",
    "input_files = [\n",
    "    \"ispd18_global_features.csv\",\n",
    "    \"ispd19_global_features.csv\",\n",
    "    \"N28_global_features.csv\"\n",
    "]\n",
    "\n",
    "df_list = [pd.read_csv(f) for f in input_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"âœ… Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Target and features\n",
    "TARGET = \"TotalWirelength\"\n",
    "y = df[TARGET]\n",
    "print(f\"ðŸŽ¯ Target variable: {TARGET}\")\n",
    "\n",
    "# %% Train-test split\n",
    "print(\"âœ‚ Splitting dataset into train and test sets...\")\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "    df[FEATURE_ORDER], y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "print(f\"âœ… Train samples: {X_train_full.shape[0]}, Test samples: {X_test_full.shape[0]}\")\n",
    "\n",
    "# %% Define models\n",
    "print(\"âš™ Initializing models...\")\n",
    "\n",
    "# Models with imputer applied to all\n",
    "models = {\n",
    "    \"LinearRegression\": wrap_model(LinearRegression()),\n",
    "    \"Ridge\": wrap_model(Ridge(random_state=SEED)),\n",
    "    \"Lasso\": wrap_model(Lasso(random_state=SEED)),\n",
    "    \"ElasticNet\": wrap_model(ElasticNet(random_state=SEED)),\n",
    "    \"LassoLars\": wrap_model(LassoLars()),\n",
    "    \"SGDRegressor\": wrap_model(SGDRegressor(random_state=SEED)),\n",
    "    \"RandomForest\": wrap_model(RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=-1)),\n",
    "    \"GradientBoosting\": wrap_model(GradientBoostingRegressor(random_state=SEED)),\n",
    "    \"ExtraTrees\": wrap_model(ExtraTreesRegressor(n_estimators=200, random_state=SEED, n_jobs=-1)),\n",
    "    \"AdaBoost\": wrap_model(AdaBoostRegressor(random_state=SEED)),\n",
    "    \"Bagging\": wrap_model(BaggingRegressor(random_state=SEED, n_jobs=-1)),\n",
    "    \"DecisionTree\": wrap_model(DecisionTreeRegressor(random_state=SEED)),\n",
    "    \"ExtraTree\": wrap_model(ExtraTreeRegressor(random_state=SEED)),\n",
    "    \"KNeighbors\": wrap_model(KNeighborsRegressor()),\n",
    "    \"XGBoost\": wrap_model(XGBRegressor(n_estimators=300, learning_rate=0.05, random_state=SEED, n_jobs=-1)),\n",
    "    \"LightGBM\": wrap_model(LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=SEED, n_jobs=-1)),\n",
    "}\n",
    "\n",
    "# Weighted Voting with wrapped models\n",
    "weights = {\"ExtraTrees\": 0.25, \"RandomForest\": 0.25, \"XGBoost\": 0.25, \"LightGBM\": 0.25}\n",
    "voting_models = [(name, models[name]) for name in weights.keys()]\n",
    "models[\"WeightedVoting\"] = VotingRegressor(voting_models, weights=list(weights.values()), n_jobs=-1)\n",
    "\n",
    "# Stacking with wrapped models\n",
    "stacking_estimators = [(name, models[name]) for name in [\"RandomForest\", \"ExtraTrees\", \"XGBoost\", \"LightGBM\"]]\n",
    "models[\"Stacking\"] = StackingRegressor(estimators=stacking_estimators, final_estimator=LinearRegression(), n_jobs=-1)\n",
    "\n",
    "print(f\"âœ… Total models: {len(models)}\")\n",
    "\n",
    "# %% Function: Evaluation\n",
    "def evaluate_and_save(model_name, model, X_train, X_test, y_train, y_test):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    infer_time = (time.time() - start_infer) / len(y_test)  # per sample\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    pred_df = pd.DataFrame({\"TrueValue\": y_test.values, \"PredictedValue\": y_pred})\n",
    "    pred_df.to_csv(os.path.join(BASE_RESULTS, \"predictions\", f\"{model_name}_predictions.csv\"), index=False)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k', s=40)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel(\"True TotalWirelength\")\n",
    "    plt.ylabel(\"Predicted TotalWirelength\")\n",
    "    plt.title(f\"{model_name}: True vs Predicted\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(BASE_RESULTS, \"plots\", f\"{model_name}_true_vs_pred.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"TrainTime(s)\": train_time,\n",
    "        \"InferTimePerSample(s)\": infer_time,\n",
    "        \"TrainSamples\": len(y_train),\n",
    "        \"TestSamples\": len(y_test)\n",
    "    }\n",
    "\n",
    "# %% Loop over feature subsets\n",
    "print(\"ðŸš€ Starting training for feature subsets...\")\n",
    "comparison_records = []\n",
    "\n",
    "for k in range(1, len(FEATURE_ORDER) + 1):\n",
    "    print(f\"\\n=== Training with top {k} features: {FEATURE_ORDER[:k]} ===\")\n",
    "    \n",
    "    X_train = X_train_full[FEATURE_ORDER[:k]]\n",
    "    X_test = X_test_full[FEATURE_ORDER[:k]]\n",
    "    \n",
    "    metrics_list = []\n",
    "    for name, model in models.items():\n",
    "        print(f\"   â–¶ {name}...\")\n",
    "        metrics = evaluate_and_save(name, model, X_train, X_test, y_train, y_test)\n",
    "        metrics_list.append(metrics)\n",
    "        \n",
    "        # Save RÂ² and MSE for comparison table\n",
    "        comparison_records.append({\n",
    "            \"NumFeatures\": k,\n",
    "            \"Model\": name,\n",
    "            f\"R2@{k}\": metrics[\"R2\"],\n",
    "            f\"MSE@{k}\": metrics[\"MSE\"]\n",
    "        })\n",
    "    \n",
    "    # Save metrics for this feature count\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    metrics_df.to_csv(os.path.join(BASE_RESULTS, \"metrics\", f\"metrics_{k}_features.csv\"), index=False)\n",
    "    print(f\"ðŸ“„ Saved metrics for {k} features\")\n",
    "\n",
    "# %% Save RÂ²/MSE comparison table\n",
    "print(\"\\nðŸ“Š Saving RÂ²/MSE comparison table...\")\n",
    "comparison_df = pd.DataFrame(comparison_records)\n",
    "comparison_df = comparison_df.pivot(index=\"Model\", columns=\"NumFeatures\")\n",
    "comparison_df.to_csv(os.path.join(BASE_RESULTS, \"metrics\", \"r2_mse_comparison.csv\"))\n",
    "print(\"âœ… All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
